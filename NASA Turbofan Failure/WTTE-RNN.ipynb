{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as k\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_loglike_discrete(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "    \n",
    "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
    "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
    "    \n",
    "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(ab):\n",
    "    a = k.exp(ab[:, 0])\n",
    "    b = k.softplus(ab[:, 1])\n",
    "    \n",
    "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "    \n",
    "    return k.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(name):\n",
    "    with open(name, 'r') as file:\n",
    "        return np.loadtext(file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'unit_number'\n",
    "time_col = 'time'\n",
    "feature_cols = [ 'op_setting_1', 'op_setting_2', 'op_setting_3'] + ['sensor_measurement_{}'.format(x) for x in range(1,22)]\n",
    "column_names = [id_col, time_col] + feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, threshold=10000)\n",
    "\n",
    "train_orig = pd.read_csv('https://raw.githubusercontent.com/daynebatten/keras-wtte-rnn/master/train.csv', header=None, names=column_names)\n",
    "test_x_orig = pd.read_csv('https://raw.githubusercontent.com/daynebatten/keras-wtte-rnn/master/test_x.csv', header=None, names=column_names)\n",
    "test_y_orig = pd.read_csv('https://raw.githubusercontent.com/daynebatten/keras-wtte-rnn/master/test_y.csv', header=None, names=['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_original = pd.concat([train_orig, test_x_orig])\n",
    "\n",
    "scaler = pipeline.Pipeline(steps=[('minmax', MinMaxScaler(feature_range=(-1, 1))),\n",
    "                                 ('remove_constant', VarianceThreshold())\n",
    "                                 ])\n",
    "all_data = all_data_original.copy()\n",
    "all_data = np.concatenate([all_data[['unit_number', 'time']], scaler.fit_transform(all_data[feature_cols\n",
    "                                                                                           ])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[0:train_orig.shape[0], :]\n",
    "test = all_data[train_orig.shape[0]:, :]\n",
    "\n",
    "train[:, 0:2] -= 1\n",
    "test[:, 0:2] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(engine, time, x, max_time, is_test, mask_value):\n",
    "    out_y = []\n",
    "    \n",
    "    d = x.shape[1]\n",
    "    \n",
    "    out_x = []\n",
    "    \n",
    "    n_engines = 100\n",
    "    for i in range(n_engines):\n",
    "        max_engine_time = int(np.max(time[engine == i])) + 1\n",
    "        \n",
    "        if is_test:\n",
    "            start = max_engine_time -1 \n",
    "        else:\n",
    "            start = 0\n",
    "        \n",
    "        this_x = []\n",
    "        \n",
    "        for j in range(start, max_engine_time):\n",
    "            engine_x = x[engine == i]\n",
    "            out_y.append(np.array((max_engine_time-j, 1), ndmin=2))\n",
    "            \n",
    "            xtemp = np.zeros((1, max_time, d))\n",
    "            xtemp += mask_value\n",
    "            \n",
    "            xtemp[:, max_time-min(j,99)-1:max_time,:] = engine_x[max(0,j-max_time+1):j+1,:]\n",
    "            this_x.append(xtemp)\n",
    "        this_x = np.concatenate(this_x)\n",
    "        out_x.append(this_x)\n",
    "    out_x = np.concatenate(out_x)\n",
    "    out_y = np.concatenate(out_y)\n",
    "    return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 100\n",
    "mask_value = -99\n",
    "\n",
    "train_x, train_y = build_data(engine=train[:,0], time=train[:,1], x=train[:,2:], \n",
    "                              max_time=max_time, is_test=False, mask_value=mask_value)\n",
    "test_x, _ = build_data(engine=test[:,0], time=test[:,1], x=test[:,2:],\n",
    "                      max_time=max_time, is_test=True, mask_value=mask_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_y_orig.copy()\n",
    "test_y['E'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (20631, 100, 17) train_y (20631, 2) test_x (100, 100, 17) test_y (100, 2)\n"
     ]
    }
   ],
   "source": [
    "print('train_x', train_x.shape,'train_y',train_y.shape, 'test_x', test_x.shape, 'test_y', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tte_mean_train 108.80786195530997 init_alpha:  109.30709957832491 mean uncensored train:  1.0\n"
     ]
    }
   ],
   "source": [
    "tte_mean_train = np.nanmean(train_y[:,0])\n",
    "mean_u = np.nanmean(train_y[:,1])\n",
    "\n",
    "init_alpha = -1.0/np.log(1.0 - 1.0/(tte_mean_train+1.0))\n",
    "init_alpha = init_alpha/mean_u\n",
    "\n",
    "print('tte_mean_train', tte_mean_train, 'init_alpha: ',init_alpha,'mean uncensored train: ',mean_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_x.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=mask_value, input_shape=(None, n_features)))\n",
    "model.add(GRU(20, activation='tanh', recurrent_dropout=0.25))\n",
    "\n",
    "# Prediction layer for alph and beta of Weibull distribution\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Apply custom activation function for alpha and beta\n",
    "model.add(Activation(activate))\n",
    "model.compile(loss=weibull_loglike_discrete, optimizer=RMSprop(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_9 (Masking)          (None, None, 17)          0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 20)                2280      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,322\n",
      "Trainable params: 2,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20631 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "20631/20631 [==============================] - 42s 2ms/step - loss: 6.8617 - val_loss: 4.6776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229d23db438>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "         epochs=1,\n",
    "         batch_size=100,\n",
    "         verbose=1,\n",
    "         validation_data=(test_x, test_y)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20631 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.9681 - val_loss: 4.5260\n",
      "Epoch 2/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.8619 - val_loss: 4.4657\n",
      "Epoch 3/100\n",
      "20631/20631 [==============================] - 28s 1ms/step - loss: 4.7929 - val_loss: 4.2924\n",
      "Epoch 4/100\n",
      "20631/20631 [==============================] - 28s 1ms/step - loss: 4.7201 - val_loss: 4.3618\n",
      "Epoch 5/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.6722 - val_loss: 4.2719\n",
      "Epoch 6/100\n",
      "20631/20631 [==============================] - 28s 1ms/step - loss: 4.6344 - val_loss: 4.2467\n",
      "Epoch 7/100\n",
      "20631/20631 [==============================] - 28s 1ms/step - loss: 4.5954 - val_loss: 4.1229\n",
      "Epoch 8/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.5706 - val_loss: 4.2826\n",
      "Epoch 9/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.5472 - val_loss: 4.0597\n",
      "Epoch 10/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.5216 - val_loss: 4.1080\n",
      "Epoch 11/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.5089 - val_loss: 4.0884\n",
      "Epoch 12/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.4968 - val_loss: 4.1094\n",
      "Epoch 13/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4836 - val_loss: 4.0911\n",
      "Epoch 14/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4738 - val_loss: 4.0616\n",
      "Epoch 15/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.4623 - val_loss: 4.0211\n",
      "Epoch 16/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4570 - val_loss: 4.1689\n",
      "Epoch 17/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.4424 - val_loss: 4.0664\n",
      "Epoch 18/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4370 - val_loss: 4.0286\n",
      "Epoch 19/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4272 - val_loss: 4.1264\n",
      "Epoch 20/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4185 - val_loss: 4.0658\n",
      "Epoch 21/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.4206 - val_loss: 4.0301\n",
      "Epoch 22/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.4055 - val_loss: 4.1248\n",
      "Epoch 23/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.4002 - val_loss: 4.0537\n",
      "Epoch 24/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.3929 - val_loss: 4.0504\n",
      "Epoch 25/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.3838 - val_loss: 4.0401\n",
      "Epoch 26/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.3795 - val_loss: 4.1407\n",
      "Epoch 27/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3759 - val_loss: 4.0747\n",
      "Epoch 28/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3668 - val_loss: 4.0642\n",
      "Epoch 29/100\n",
      "20631/20631 [==============================] - 32s 2ms/step - loss: 4.3597 - val_loss: 4.0982\n",
      "Epoch 30/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3622 - val_loss: 4.0546\n",
      "Epoch 31/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.3528 - val_loss: 4.0763\n",
      "Epoch 32/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.3455 - val_loss: 4.0186\n",
      "Epoch 33/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3396 - val_loss: 4.2024\n",
      "Epoch 34/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3324 - val_loss: 4.1943\n",
      "Epoch 35/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.3321 - val_loss: 3.9979\n",
      "Epoch 36/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.3203 - val_loss: 4.0363\n",
      "Epoch 37/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3212 - val_loss: 4.0101\n",
      "Epoch 38/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3176 - val_loss: 3.9977\n",
      "Epoch 39/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3091 - val_loss: 4.2173\n",
      "Epoch 40/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.3038 - val_loss: 4.0035\n",
      "Epoch 41/100\n",
      "20631/20631 [==============================] - 32s 2ms/step - loss: 4.3004 - val_loss: 4.0772\n",
      "Epoch 42/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2917 - val_loss: 4.0283\n",
      "Epoch 43/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.2884 - val_loss: 3.9851\n",
      "Epoch 44/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2846 - val_loss: 3.9887\n",
      "Epoch 45/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2766 - val_loss: 4.0894\n",
      "Epoch 46/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2714 - val_loss: 4.0130\n",
      "Epoch 47/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2635 - val_loss: 4.0523\n",
      "Epoch 48/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2625 - val_loss: 4.0254\n",
      "Epoch 49/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2525 - val_loss: 4.0055\n",
      "Epoch 50/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.2440 - val_loss: 4.1102\n",
      "Epoch 51/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2461 - val_loss: 3.9994\n",
      "Epoch 52/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2445 - val_loss: 4.0265\n",
      "Epoch 53/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2432 - val_loss: 4.1066\n",
      "Epoch 54/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2291 - val_loss: 4.0601\n",
      "Epoch 55/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.2334 - val_loss: 4.0166\n",
      "Epoch 56/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.2209 - val_loss: 4.0348\n",
      "Epoch 57/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2153 - val_loss: 4.0754\n",
      "Epoch 58/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.2104 - val_loss: 4.1680\n",
      "Epoch 59/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.2114 - val_loss: 4.0287\n",
      "Epoch 60/100\n",
      "20631/20631 [==============================] - 32s 2ms/step - loss: 4.2096 - val_loss: 4.0353\n",
      "Epoch 61/100\n",
      "20631/20631 [==============================] - 34s 2ms/step - loss: 4.2038 - val_loss: 4.2915\n",
      "Epoch 62/100\n",
      "20631/20631 [==============================] - 32s 2ms/step - loss: 4.1931 - val_loss: 4.0789\n",
      "Epoch 63/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.1896 - val_loss: 4.0483\n",
      "Epoch 64/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.1830 - val_loss: 4.1636\n",
      "Epoch 65/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.1849 - val_loss: 4.1117\n",
      "Epoch 66/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.1820 - val_loss: 4.1785\n",
      "Epoch 67/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.1776 - val_loss: 4.0497\n",
      "Epoch 68/100\n",
      "20631/20631 [==============================] - 34s 2ms/step - loss: 4.1634 - val_loss: 4.2702\n",
      "Epoch 69/100\n",
      "20631/20631 [==============================] - 34s 2ms/step - loss: 4.1648 - val_loss: 4.2151\n",
      "Epoch 70/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.1610 - val_loss: 4.1039\n",
      "Epoch 71/100\n",
      "20631/20631 [==============================] - 33s 2ms/step - loss: 4.1606 - val_loss: 4.0948\n",
      "Epoch 72/100\n",
      "20631/20631 [==============================] - 34s 2ms/step - loss: 4.1556 - val_loss: 4.1187\n",
      "Epoch 73/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.1464 - val_loss: 4.1359\n",
      "Epoch 74/100\n",
      "20631/20631 [==============================] - 36s 2ms/step - loss: 4.1418 - val_loss: 4.1036\n",
      "Epoch 75/100\n",
      "20631/20631 [==============================] - 31s 1ms/step - loss: 4.1450 - val_loss: 4.0941\n",
      "Epoch 76/100\n",
      "20631/20631 [==============================] - 32s 2ms/step - loss: 4.1337 - val_loss: 4.1829\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.1314 - val_loss: 4.1300\n",
      "Epoch 78/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.1305 - val_loss: 4.1708\n",
      "Epoch 79/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.1265 - val_loss: 4.1302\n",
      "Epoch 80/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.1209 - val_loss: 4.3246\n",
      "Epoch 81/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.1192 - val_loss: 4.1266\n",
      "Epoch 82/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.1126 - val_loss: 4.4299\n",
      "Epoch 83/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.1118 - val_loss: 4.2292\n",
      "Epoch 84/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.1060 - val_loss: 4.4123\n",
      "Epoch 85/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.1072 - val_loss: 4.2373\n",
      "Epoch 86/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.0982 - val_loss: 4.2055\n",
      "Epoch 87/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.0979 - val_loss: 4.2312\n",
      "Epoch 88/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0956 - val_loss: 4.1939\n",
      "Epoch 89/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.0967 - val_loss: 4.1920\n",
      "Epoch 90/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0877 - val_loss: 4.3271\n",
      "Epoch 91/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0873 - val_loss: 4.2374\n",
      "Epoch 92/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0847 - val_loss: 4.6822\n",
      "Epoch 93/100\n",
      "20631/20631 [==============================] - 30s 1ms/step - loss: 4.0925 - val_loss: 4.2776\n",
      "Epoch 94/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0812 - val_loss: 4.2533\n",
      "Epoch 95/100\n",
      "20631/20631 [==============================] - 35s 2ms/step - loss: 4.0733 - val_loss: 4.3669\n",
      "Epoch 96/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0722 - val_loss: 4.3717\n",
      "Epoch 97/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0753 - val_loss: 4.4864\n",
      "Epoch 98/100\n",
      "20631/20631 [==============================] - 29s 1ms/step - loss: 4.0600 - val_loss: 4.5596\n",
      "Epoch 99/100\n",
      "20631/20631 [==============================] - 31s 2ms/step - loss: 4.0766 - val_loss: 4.2855\n",
      "Epoch 100/100\n",
      "20631/20631 [==============================] - 475s 23ms/step - loss: 4.0594 - val_loss: 4.3172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229d3c93a90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "         epochs=100,\n",
    "         batch_size=100,\n",
    "         verbose=1,\n",
    "         validation_data=(test_x, test_y)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_x)\n",
    "test_predict = np.resize(test_predict, (100,2))\n",
    "test_result = np.concatenate((test_y, test_predict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = pd.DataFramerame(test_result, columns=['T','E', 'alpha','beta'])\n",
    "test_result_df['unit_number'] = np.arange(1, test_results_df.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
